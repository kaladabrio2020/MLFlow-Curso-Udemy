
## Experimentos
Um experimento pode ter um $n$ n√∫mero de execu√ß√µes, onde uma excu√ß√£o √© um unica execu√ß√£o de um peda√ßo de c√≥digo ou um m√≥delo de aprendizado de m√°quina.

**Cada execu√ß√£o pode registrar vers√£o do c√≥digo, hiperparametros, trilhas de matriz, arterfator** isso tudo √© armazenado no MLFlow.

Experiementos s√£o agrupamentos l√≥gicos de execu√ß√µes. Que nos permitem organizar e analisar grupos de execu√ß√µes.
  
**Sempre quanto cria e exceta um experiemento ele recebe um id=pr√≥prio** e um nome que pode ser utilizado para recuperar metadados

### Criando experimentos

```python
# Cria um novo experimento
mlflow.create_experiment() # ela retorna o id do experimento
```
Parametros:
* `name` = defini o nome experimento **deve ser √∫nico**
* `artifact_location` = defini o local dos artefartos onde ficar√° armazenados . **Opicional**
* ¬†`tags` = ¬†mais adiante

```python
# Defini um novo experimento j√° existente
mlflow.set_experiment()
```
## Logging functions

S√£o fun√ß√µes utilizadas para registrar (logar) informa√ß√µes relevantes durante o treinamento e avalia√ß√£o de modelos de aprendizado de m√°quina. Essas informa√ß√µes incluem m√©tricas, par√¢metros, artefatos (como gr√°ficos ou arquivos) e modelos, permitindo rastrear experimentos e facilitar reprodutibilidade.

1. As duas primeiras s√£o :

¬† ¬† * Uri de rastramento de conj. de pontos do MLflow e Uri de rastramento de ponto do MLflow

¬† ¬† ```python
mlflow.set_tracking_uri()
¬† ¬† ```

¬† ¬† > √â usada para definir o local de sua escolha, onde deseja manter os rastros de seu codigo. A
  
```python
mlflow.get_tracking_uri()
```

¬† ¬† > √â recuperar o caminho da localiza√ß√£o

exemplo abaixo:
```python
mlflow.set_tracking_uri(
¬† ¬† uri="./works"
)

print("Uri :", mlflow.get_tracking_uri())
try:
¬† ¬† exp = mlflow.create_experiment(
¬† ¬† ¬† ¬† name = "GradientBoss",
¬† ¬† ¬† ¬† tags = dict(version_ = "1.0.0")
¬† ¬† )
¬† ¬† get_ = mlflow.get_experiment(experiment_id=exp)

except mlflow.exceptions.MlflowException as e:
¬† ¬† get_ = mlflow.set_experiment(experiment_name="GradientBoss")
```

## Auto logging

√â uma ferramenta que permite logging automatico de certos parametros, metricas, artifacts. Sem a necessdade de instrumenta√ß√£o explicita do c√≥digo.
>  Bom quando o projeto est√° ficando grande.

* `mlflow.autolog` : Auto logging generico para qualquer lib
* `mlflow.<lib>.autolog` : autologo especifico para algumas libs

Parametros:
1. log_models : log de model ou n√£o
2. log_input_example : se definir True ele os exemplos de entrado para treinamento 

## Tracking Server

√â centraliza√ß√£o de reposit√≥rio que armazena metadados e gera√ß√£o de artefatos durante o treinamento de modelos de ml.
* Permite compartilhar resultados com a equipe
Existem dois tipos servidores:
1. Storage : Permite armazenar os artefatos, metadados gerados durante o processo de treinamento
	1. Backend : Usa bando de dados e arquivo de amazenamento
		* SQLlite, Azure, GCP
		 *  Armazena metadados, nome, id, paramentros, metricas , tags, run name e etc
	2. Artefato storage 
		* Input data, output data, modelos ou visuals
		* Local
		
2. Network : todos os usuarios interagem com tracking server usando api rest ou requisi√ß√£o
	1. Rest Api
		* Oferece uma interface simples e flexivel para acessar o servidor de rastreamento por http
	2. RPC
		*  Comunica√ß√£o bidirecional, mais rapido
	*
Exemplo abaixo usando SQLlite como backend para salvar os dados
	
```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mytracks --host 127.0.0.1
```
> `mlflow.set_tracking_uri(uri="http://127.0.0.1:5000")`
> **Remova depois de rodar o c√≥digo o ` --default-artifact-root ./mytracks`**

## Model Compenent

√â um formato padr√£o para para empacotar seu modelo de aprendizado de m√°quina em um formato reutiliz√°vel. 
> Empacotados em uma formato adequado por exemplo pickle.


* Model Signature: Especifica entrada e saida
	* √â uma forma de descrever os tipos de dados de entrada e saida e as formas esperadas e produzidas por um modelo de aprendizado de maquina.
* Model API : Podemos gerar uma api com muita facilidade , e interagir com o model por meio de uma interface
	* REST API : Flask, ou ferramentas python
	* Poder fazer o deploy em diversos sistemas - cloud, on-premises servers, devices
* Flavor : Uma variante refere-se a uma maneira especifica de serializar e armazenar um modelo de aprendizado de m√°quina.


---

# üìå `mlflow.evaluate()`

**Defini√ß√£o:**  
A fun√ß√£o `mlflow.evaluate()` permite **avaliar um modelo registrado no MLflow** usando um dataset de teste, calculando m√©tricas automaticamente, gerando gr√°ficos de desempenho e salvando artefatos de avalia√ß√£o no MLflow Tracking.

**Par√¢metros principais:**

- `model`: URI do modelo registrado no MLflow (`runs:/<run_id>/<artifact_path>`), objeto PyFunc, ou fun√ß√£o custom de predi√ß√£o.
    
- `data`: DataFrame contendo **features + target**.
    
- `targets`: Nome da coluna de target no DataFrame.
    
- `model_type`: Tipo do modelo: `"classifier"` ou `"regressor"`.
    
- `evaluator_config` (opcional): Configura√ß√µes extras, como lista de labels para classifica√ß√£o multiclass.
    

**Retorno:**  
Um objeto `EvaluationResult` com m√©tricas (`metrics`), artefatos (`artifacts`) e informa√ß√µes sobre o dataset.

---

## üß™ Exemplo simples com classifica√ß√£o

```python
import mlflow
import mlflow.sklearn
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import pandas as pd

# Dataset
data = load_wine(as_frame=True)['data']
target = load_wine(as_frame=True)['target']
X = pd.concat([data, target.rename("target")], axis=1)

train_df, test_df = train_test_split(X, test_size=0.25, random_state=42)
X_train, y_train = train_df.drop(columns=["target"]), train_df["target"]

with mlflow.start_run() as run:
    # Treinar e logar modelo
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)
    mlflow.sklearn.log_model(model, artifact_path="model")

    # Obter URI do modelo logado
    model_uri = f"runs:/{run.info.run_id}/model"

    # Avaliar modelo
    eval_result = mlflow.evaluate(
        model=model_uri,
        data=test_df,
        targets="target",
        model_type="classifier",
        evaluator_config={"label_list": [0, 1, 2]}
    )

print(eval_result.metrics)
```

**‚úÖ Sa√≠da esperada:**

```python
{'accuracy': 0.955, 'f1_score': 0.954, 'recall': 0.95}
```

---

Se quiser, posso fazer **uma vers√£o em Markdown ainda mais resumida**, tipo ‚Äúquick reference‚Äù para usar `mlflow.evaluate()` em qualquer projeto de classifica√ß√£o ou regress√£o.

Quer que eu fa√ßa?